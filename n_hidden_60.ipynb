{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "n hidden 60.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrZg_4b_ryA1",
        "colab_type": "code",
        "outputId": "21ed88ec-d8ea-4249-a015-f991f71ecb66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "!git clone https://github.com/afrinaSchool/dataset.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dataset'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)   \u001b[K\rremote: Counting objects:  28% (2/7)   \u001b[K\rremote: Counting objects:  42% (3/7)   \u001b[K\rremote: Counting objects:  57% (4/7)   \u001b[K\rremote: Counting objects:  71% (5/7)   \u001b[K\rremote: Counting objects:  85% (6/7)   \u001b[K\rremote: Counting objects: 100% (7/7)   \u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 7 (delta 1), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (7/7), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZFQXAtksDYx",
        "colab_type": "code",
        "outputId": "dccf8f08-9173-4c68-ebe6-bc725749299d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2gy-dS4sHsb",
        "colab_type": "code",
        "outputId": "c64086bb-b11a-48f5-f0a9-f7a5ae429e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GoogleNews-vectors-negative300.bin.gz  train.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8a5mCcwsWyU",
        "colab_type": "code",
        "outputId": "5978f5d1-570c-40a9-bc63-5da1c23a744c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "file_name = \"train.csv.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                                             Modified             Size\n",
            "train.csv                                      2017-03-08 07:34:06     63399110\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AkxYj3IseWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "train_df, test_df = train_test_split(train_df, test_size = 0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pidZ-6ontSww",
        "colab_type": "code",
        "outputId": "dfdb4779-980e-45b3-811e-6305e0b19b17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(1234)\n",
        "from gensim.models import KeyedVectors\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import datetime\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import nltk\n",
        "from gensim.test.utils import datapath\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jRD0xLdtbms",
        "colab_type": "code",
        "outputId": "487e2975-3197-495f-dd1f-262a2b719e35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "file_name = \"/content/gdrive/My Drive/GoogleNews-vectors-negative300.bin.gz\"\n",
        "#file_name=\"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?usp=sharing\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTsj0sOMt0mK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "def text_to_word_list(text):\n",
        "    ''' Pre process and convert texts to a list of words '''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text) \n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5OhstE1vPEY",
        "colab_type": "code",
        "outputId": "9815f5c4-800e-41fb-a4a7-95693b88592a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Prepare embedding\n",
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
        "#word2vec =  KeyedVectors.load_word2vec_format(datapath(\"euclidean_vectors.bin\"), binary=True,size=300) \n",
        "\n",
        "word2vec =  KeyedVectors.load_word2vec_format(file_name,binary=True) \n",
        "\n",
        "questions_cols = ['question1', 'question2']\n",
        "\n",
        "# Iterate over the questions only of both training and test datasets\n",
        "for dataset in [train_df, test_df]:\n",
        "    for index, row in dataset.iterrows():\n",
        "\n",
        "        # Iterate through the text of both questions of the row\n",
        "        for question in questions_cols:\n",
        "\n",
        "            q2n = []  # q2n -> question numbers representation\n",
        "            for word in text_to_word_list(row[question]):\n",
        "\n",
        "                # Check for unwanted words\n",
        "                if word in stops and word not in word2vec.vocab:\n",
        "                    continue\n",
        "\n",
        "                if word not in vocabulary:\n",
        "                    vocabulary[word] = len(inverse_vocabulary)\n",
        "                    q2n.append(len(inverse_vocabulary))\n",
        "                    inverse_vocabulary.append(word)\n",
        "                else:\n",
        "                    q2n.append(vocabulary[word])\n",
        "\n",
        "            # Replace questions as word to question as number representation\n",
        "            dataset.set_value(index, question, q2n)\n",
        "            \n",
        "embedding_dim = 300 #Google w2v word dimension is 300\n",
        "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "# Build the embedding matrix\n",
        "for word, index in vocabulary.items():\n",
        "    if word in word2vec.vocab:\n",
        "        embeddings[index] = word2vec.word_vec(word)\n",
        "\n",
        "del word2vec\n",
        "\n",
        "\n",
        "max_seq_length = max(train_df.question1.map(lambda x: len(x)).max(),\n",
        "                     train_df.question2.map(lambda x: len(x)).max(),\n",
        "                     test_df.question1.map(lambda x: len(x)).max(),\n",
        "                     test_df.question2.map(lambda x: len(x)).max())\n",
        "\n",
        "# Split to train validation\n",
        "validation_size = 400\n",
        "training_size = len(train_df) - validation_size\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6sb8IDdD6wR",
        "colab_type": "text"
      },
      "source": [
        "Train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDtdNcYCD1GQ",
        "colab_type": "code",
        "outputId": "18adf613-a5e7-446d-bfda-64ab8cf2cadd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "\n",
        "\n",
        "X = train_df[questions_cols]\n",
        "Y = train_df['is_duplicate']\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
        "\n",
        "# Split to dicts\n",
        "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
        "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
        "X_test = {'left': test_df.question1, 'right': test_df.question2}\n",
        "Y_test=test_df['is_duplicate']\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "Y_train = Y_train.values\n",
        "Y_validation = Y_validation.values\n",
        "\n",
        "\n",
        "# Zero padding\n",
        "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
        "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
        "\n",
        "# Make sure everything is ok\n",
        "assert X_train['left'].shape == X_train['right'].shape\n",
        "assert len(X_train['left']) == len(Y_train)\n",
        "\n",
        "#Build the model\n",
        "\n",
        "# Model variables - change kore kore dekha uchit\n",
        "n_hidden = 60\n",
        "gradient_clipping_norm = 1.25 \n",
        "batch_size = 256\n",
        "n_epoch = 12\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "# The visible layer\n",
        "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "#weight????????????????????????????????????????\n",
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
        "\n",
        "# Embedded version of the inputs\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "# Since this is a siamese network, both sides share the same LSTM\n",
        "shared_lstm = LSTM(n_hidden)\n",
        "\n",
        "left_output = shared_lstm(encoded_left)\n",
        "right_output = shared_lstm(encoded_right)\n",
        "\n",
        "# Calculates the distance as defined by the MaLSTM model\n",
        "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "# Pack it all up into a model\n",
        "malstm = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "# Adadelta optimizer, with gradient clipping by norm\n",
        "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "\n",
        "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Start training\n",
        "training_start_time = time()\n",
        "\n",
        "malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, epochs=n_epoch,\n",
        "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
        "\n",
        "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 323032 samples, validate on 400 samples\n",
            "Epoch 1/12\n",
            "323032/323032 [==============================] - 1129s 3ms/step - loss: 0.1904 - acc: 0.7203 - val_loss: 0.1781 - val_acc: 0.7375\n",
            "Epoch 2/12\n",
            "323032/323032 [==============================] - 1122s 3ms/step - loss: 0.1690 - acc: 0.7560 - val_loss: 0.1722 - val_acc: 0.7450\n",
            "Epoch 3/12\n",
            "323032/323032 [==============================] - 1121s 3ms/step - loss: 0.1621 - acc: 0.7693 - val_loss: 0.1675 - val_acc: 0.7500\n",
            "Epoch 4/12\n",
            "323032/323032 [==============================] - 1119s 3ms/step - loss: 0.1577 - acc: 0.7782 - val_loss: 0.1647 - val_acc: 0.7675\n",
            "Epoch 5/12\n",
            "323032/323032 [==============================] - 1122s 3ms/step - loss: 0.1543 - acc: 0.7835 - val_loss: 0.1631 - val_acc: 0.7550\n",
            "Epoch 6/12\n",
            "323032/323032 [==============================] - 1123s 3ms/step - loss: 0.1517 - acc: 0.7885 - val_loss: 0.1611 - val_acc: 0.7525\n",
            "Epoch 7/12\n",
            "132096/323032 [===========>..................] - ETA: 11:01 - loss: 0.1493 - acc: 0.7914"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QED8P7k8EDft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(malstm_trained.history['acc'])\n",
        "plt.plot(malstm_trained.history['val_acc'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(malstm_trained.history['loss'])\n",
        "plt.plot(malstm_trained.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_R-sDY0EWC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model_json = malstm.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "malstm.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8OIxI8kEd4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import model_from_json\n",
        "\n",
        "n_hidden = 50\n",
        "gradient_clipping_norm = 1.25 \n",
        "batch_size = 256\n",
        "n_epoch = 10\n",
        "\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "# The visible layer\n",
        "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
        "\n",
        "# Embedded version of the inputs\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "# Since this is a siamese network, both sides share the same LSTM\n",
        "shared_lstm = LSTM(n_hidden)\n",
        "\n",
        "left_output = shared_lstm(encoded_left)\n",
        "right_output = shared_lstm(encoded_right)\n",
        "\n",
        "# Calculates the distance as defined by the MaLSTM model\n",
        "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "\n",
        "malstm_loaded = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "# load weights into new model\n",
        "malstm_loaded .load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "\n",
        "malstm_loaded.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "for dataset, side in itertools.product([X_test], ['left', 'right']):\n",
        "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
        "\n",
        "scores = malstm_loaded.evaluate([X_test['left'], X_test['right']], Y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (malstm_loaded.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}